# ğŸ§ DORI â€” Multilingual Autonomous Tour Guide Robot

**DORI (ë‹¤êµ­ì–´ ê´€ê´‘ ì•ˆë‚´ ë¡œë´‡)** is an autonomous tour guide robot system designed to provide multilingual guidance to tourists visiting cultural heritage sites, specifically Gyeongbokgung Palace in Seoul, South Korea.

## ğŸ“‹ Project Overview

DORI integrates three core modules:
1. **Multilingual Q&A System** with RAG-based LLM for intelligent question answering
2. **Photographer Dori** for automated tourist photography (framework implemented)
3. **Autonomous Navigation** using sensor fusion (planned)

The system supports **8 languages** (English, Korean, Japanese, Chinese, French, Spanish, Vietnamese, Thai) and provides real-time speech-to-speech interaction with context-aware answers using Retrieval-Augmented Generation (RAG).

## âœ… Completed Features

### Core Infrastructure
- âœ… **PostgreSQL Database**: Hierarchical structure (Places â†’ Spots â†’ Scripts)
- âœ… **Knowledge Base**: 30+ knowledge documents for RAG
- âœ… **FAISS Vector Index**: Fast semantic search for context retrieval
- âœ… **Dual Embedding Models**: e5-small-v2 + gte-small for enhanced retrieval

### Multilingual Support
- âœ… **8 Languages**: English, Korean, Japanese, Chinese, French, Spanish, Vietnamese, Thai
- âœ… **Runtime Translation**: LLM-based translation pipeline
- âœ… **Translation Caching**: Optimized performance with cached translations
- âœ… **Language Auto-Detection**: Automatic detection from wakeword

### Speech Services
- âœ… **STT (Speech-to-Text)**: Google Cloud Speech-to-Text integration
- âœ… **TTS (Text-to-Speech)**: Google Cloud Text-to-Speech with natural voice synthesis
- âœ… **Multi-language Recognition**: Supports all 8 languages

### Wakeword Detection
- âœ… **Voice-based Detection**: "Hey Dori" (English) / "ë„ë¦¬ì•¼" (Korean)
- âœ… **Fuzzy Matching**: Handles pronunciation variations using Levenshtein distance
- âœ… **Language Auto-Detection**: Determines user language from wakeword
- âœ… **Cooldown Mechanism**: Prevents duplicate triggers

### Tour Loop System
- âœ… **Complete Tour Orchestration**: Sequential navigation through 6 spots
- âœ… **Spot Introduction**: TTS narration for each location
- âœ… **Q&A Sessions**: Interactive question-answering with 10-second timeout
- âœ… **Inline Wakeword Interrupt**: Users can interrupt during narration
- âœ… **Automatic Progression**: Moves to next spot if no questions

### Q&A System with RAG
- âœ… **RAG Pipeline**: FAISS-based semantic search from knowledge base
- âœ… **LLM Integration**: Local LLM via LM Studio (Llama-3.1-8B-Instruct)
- âœ… **Proper Noun Normalization**: Handles mispronunciations of palace names
- âœ… **Multi-turn Q&A**: Supports follow-up questions
- âœ… **"Pass" Command**: Users can skip questions
- âœ… **RAG Toggle**: Can enable/disable RAG via config flag

### Photo Spot Feature
- âœ… **Photo Spot Detection**: Identifies designated photo locations
- âœ… **Positioning Instructions**: Guides users to optimal positions
- âœ… **Countdown System**: 5-second countdown before capture
- âš ï¸ **Camera Integration**: Framework ready, hardware integration pending

## ğŸ—ï¸ System Architecture

### Technology Stack

| Component | Technology |
|-----------|-----------|
| **Programming Language** | Python 3.11 |
| **Database** | PostgreSQL + psycopg2 |
| **Speech Recognition** | Google Cloud Speech-to-Text |
| **Speech Synthesis** | Google Cloud Text-to-Speech |
| **LLM** | Local LLM (LM Studio / Ollama / llama.cpp) |
| **RAG** | FAISS + e5-small-v2 + gte-small embeddings |
| **Deployment** | Docker / docker-compose |
| **Hardware** | Unitree Go2 Quadruped Robot + NVIDIA Orin |

### Data Flow

**Q&A Pipeline:**
```
User Speech â†’ STT (Google) â†’ Language Detection
    â†“
Translation (User Lang â†’ English) â†’ RAG Context Retrieval
    â†“
LLM Answer Generation â†’ Translation (English â†’ User Lang)
    â†“
TTS (Google Cloud) â†’ Audio Output
```

**Tour Flow:**
```
Wakeword Detection â†’ Language Auto-Detection â†’ Greeting
    â†“
For each spot (6 spots):
    - Arrival Announcement
    - Spot Introduction (TTS)
    - Q&A Session (10s timeout)
    - Photo Spot Check (if applicable)
    â†“
Tour Completion Message
```

## ğŸ“ Project Structure

```
dori-project/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ dori_main.py              # Entry point: wakeword detection â†’ tour start
â”‚   â”œâ”€â”€ main_tour_loop.py         # Main tour orchestration logic
â”‚   â”œâ”€â”€ tour_route.py             # Tour route definition (6 spots)
â”‚   â”œâ”€â”€ stt_service.py            # Google Cloud Speech-to-Text
â”‚   â”œâ”€â”€ tts_service.py            # Google Cloud Text-to-Speech
â”‚   â”œâ”€â”€ wakeword_service.py       # Wakeword detection ("Hey Dori")
â”‚   â”œâ”€â”€ translation_service.py    # LLM-based translation
â”‚   â”œâ”€â”€ llm_client.py             # Local LLM interface (LM Studio/Ollama)
â”‚   â”œâ”€â”€ rag_pipeline.py           # RAG context building
â”‚   â”œâ”€â”€ faiss_retriever.py       # FAISS vector search
â”‚   â”œâ”€â”€ embedding_client.py      # Dual embedding models
â”‚   â”œâ”€â”€ db_utils.py              # PostgreSQL CRUD operations
â”‚   â”œâ”€â”€ config.py                # Configuration (RAG toggle, DB settings)
â”‚   â”œâ”€â”€ 00_init_db.py            # Database initialization
â”‚   â”œâ”€â”€ 01_seed_spots.py         # Seed spot data
â”‚   â”œâ”€â”€ 02_seed_knowledge_docs.py # Seed knowledge base
â”‚   â””â”€â”€ 03_build_faiss_index.py   # Build FAISS index
â”œâ”€â”€ db/
â”‚   â”œâ”€â”€ schema.sql               # Database schema
â”‚   â””â”€â”€ sample_data.sql          # Sample data
â”œâ”€â”€ faiss_index_en.bin           # FAISS vector index
â”œâ”€â”€ faiss_ids_en.npy             # FAISS document IDs
â”œâ”€â”€ requirements.txt             # Python dependencies
â””â”€â”€ README.md                    # This file
```

## ğŸš€ Setup Instructions

### Prerequisites
- Python 3.11+
- PostgreSQL database
- Google Cloud credentials for STT/TTS
- Local LLM server (LM Studio / Ollama)

### 1. Clone and Install Dependencies

```bash
# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### 2. Database Setup

```bash
# Create database
psql -U postgres -c "CREATE DATABASE dori;"

# Initialize schema
psql -U postgres -d dori -f db/schema.sql
psql -U postgres -d dori -f db/sample_data.sql
```

### 3. Configure Environment

```bash
# Set Google Cloud credentials
export GOOGLE_APPLICATION_CREDENTIALS=./credentials/gcp-service-account.json

# Update database credentials in app/config.py
# DB_HOST, DB_NAME, DB_USER, DB_PASSWORD
```

### 4. Seed Data and Build Index

```bash
# Seed spots
python app/01_seed_spots.py

# Seed knowledge documents
python app/02_seed_knowledge_docs.py

# Build FAISS index
python app/03_build_faiss_index.py
```

### 5. Start Local LLM Server

**Option A: LM Studio**
- Download and install LM Studio
- Load model: `Llama-3.1-8B-Instruct-GGUF`
- Start local server on `http://127.0.0.1:1234`

**Option B: Ollama**
```bash
ollama pull llama3.1:8b
ollama serve
```

### 6. Run the Application

```bash
python app/dori_main.py
```

## ğŸ¯ Usage

### Starting a Tour

1. **Wakeword Activation**: Say "Hey Dori" (English) or "ë„ë¦¬ì•¼" (Korean)
2. **Language Detection**: System automatically detects your language
3. **Tour Begins**: Robot greets you and starts the tour

### During the Tour

- **Spot Introductions**: Robot narrates information about each location
- **Q&A Sessions**: Ask questions after each spot introduction
  - Wait for "Do you have any questions?" prompt
  - Ask your question (10-second timeout)
  - Robot answers using RAG + LLM
  - Say "pass" to skip questions
- **Photo Spots**: At designated locations, robot will guide you for photos

### Tour Route

The tour visits 6 spots in order:
1. **Gwanghwamun** (ê´‘í™”ë¬¸) - Main gate
2. **Heungnyemun** (í¥ë¡€ë¬¸) - Second gate
3. **Geunjeongmun** (ê·¼ì •ë¬¸) - Third gate
4. **Geunjeongjeon** (ê·¼ì •ì „) - Main throne hall
5. **Sujeongjeon** (ìˆ˜ì •ì „) - Discussion hall
6. **Gyeonghoeru** (ê²½íšŒë£¨) - Photo spot pavilion

## âš™ï¸ Configuration

### RAG Toggle

Edit `app/config.py` to enable/disable RAG:

```python
ENABLE_RAG = True   # Use knowledge base for context-aware answers
ENABLE_RAG = False  # Use LLM general knowledge only
```

**When RAG is enabled:**
- Answers use context from knowledge_docs
- More accurate, site-specific information
- Better handling of historical/cultural questions

**When RAG is disabled:**
- LLM uses general knowledge only
- Faster response (no retrieval step)
- Useful for comparing answer quality

### Database Configuration

Update `app/config.py` with your database credentials:

```python
DB_HOST = "localhost"
DB_NAME = "dori"
DB_USER = "postgres"
DB_PASSWORD = "your_password"
```

## ğŸ“Š Current Status

### Completed: ~85%
- âœ… Core infrastructure and database
- âœ… Multilingual support system (8 languages)
- âœ… Speech services (STT/TTS)
- âœ… Wakeword detection
- âœ… Tour loop and navigation
- âœ… Q&A with RAG
- âœ… Knowledge base (30+ documents)
- âœ… Photo spot framework

### In Progress: ~15%
- â³ Hardware integration (navigation, camera)
- â³ Enhanced wakeword (Porcupine/Whisper)
- â³ Production deployment on Unitree Go2
- â³ Sensor fusion for GPS-based navigation

## ğŸ”® Future Work

### Priority 1: Hardware Integration
- GPS-based autonomous navigation
- Camera integration for photo capture
- Unitree Go2 control system integration

### Priority 2: Production Readiness
- Deploy on Unitree Go2 + NVIDIA Orin
- Performance optimization
- Enhanced error handling

### Priority 3: Feature Enhancement
- Porcupine wakeword integration
- Enhanced RAG context filtering
- User feedback analysis system
- Support for additional languages

## ğŸ§ª Testing RAG Utility

To compare RAG-enabled vs RAG-disabled responses:

1. **Enable RAG**: Set `ENABLE_RAG = True` in `config.py`
2. **Test Questions**: Ask site-specific questions (e.g., "When was Geunjeongjeon built?")
3. **Disable RAG**: Set `ENABLE_RAG = False`
4. **Test Same Questions**: Compare answer quality and accuracy

## ğŸ“ Key Design Decisions

1. **English as Source Language**: All content stored in English, translated at runtime
2. **RAG for Q&A**: Ensures accurate, context-aware answers from knowledge base
3. **Local LLM**: Privacy and offline capability
4. **Modular Architecture**: Easy to extend with new spots, languages, or features
5. **Proper Noun Normalization**: Handles mispronunciations of palace names

## ğŸ¤ Contributing

This is a graduation project. For questions or contributions, please contact the project team.

## ğŸ“„ License

[Specify your license here]

## ğŸ™ Acknowledgments

- **Q&A & Multilingual System**: [Team Member]
- **Photography Module**: Minseo
- **Autonomous Navigation**: [Team Member]

---

**Institution**: [Your University/Institution]  
**Date**: 2024
