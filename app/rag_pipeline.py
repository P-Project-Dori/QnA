# rag_pipeline.py

from faiss_retriever import FaissRetriever
from db_utils import get_scripts_for_spot_code, get_knowledge_docs_by_ids
from config import ENABLE_RAG

# ============================================================================
# RAG Retriever Initialization
# ============================================================================
# FAISS retriever is initialized here but will only be used when ENABLE_RAG = True
# When RAG is disabled, this retriever won't be called, saving resources
# ============================================================================
retriever = FaissRetriever(
    index_path="faiss_index_en.bin",
    ids_path="faiss_ids_en.npy",
    top_k=5,
)


def build_rag_context(question: str, spot_code=None):
    """
    RAGìš© ì»¨í…ìŠ¤íŠ¸ ìƒì„±
    
    Args:
        question: ì‚¬ìš©ì ì§ˆë¬¸
        spot_code: ìŠ¤íŒŸ ì½”ë“œ (í•„í„°ë§ìš©, í˜„ì¬ ë¯¸êµ¬í˜„)
    
    Returns:
        RAG ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ (RAGê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆê±°ë‚˜ ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ë¬¸ìì—´)
    
    Process:
        1) ENABLE_RAG í”Œë˜ê·¸ í™•ì¸
        2) FAISSë¡œ ë¬¸ì„œ ê²€ìƒ‰ (knowledge_doc IDs ë°˜í™˜)
        3) DBì—ì„œ ì‹¤ì œ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
        4) spot_code í•„í„°ë§ (ìˆì„ ê²½ìš°, í–¥í›„ êµ¬í˜„)
    
    Error Handling:
        - RAG ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜í•˜ì—¬ LLM-only ëª¨ë“œë¡œ ìë™ ì „í™˜
        - CUDA ë©”ëª¨ë¦¬ ì˜¤ë¥˜, ì„ë² ë”© ëª¨ë¸ ì˜¤ë¥˜ ë“± ëª¨ë“  ì˜ˆì™¸ ì²˜ë¦¬
    """
    # ========================================================================
    # RAG TOGGLE CHECK: If RAG is disabled, return empty context
    # ========================================================================
    if not ENABLE_RAG:
        print("â„¹ï¸  [RAG] RAG is disabled (ENABLE_RAG = False)")
        return ""
    
    # ========================================================================
    # RAG ENABLED: Proceed with context retrieval (with error handling)
    # ========================================================================
    try:
        # FAISS ê²€ìƒ‰ìœ¼ë¡œ knowledge_doc IDs ì–»ê¸°
        doc_ids = retriever.search(question)
        print(f"ğŸ” [RAG] Retrieved {len(doc_ids)} document IDs: {doc_ids}")
        
        # DBì—ì„œ ì‹¤ì œ ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
        if not doc_ids:
            print("âš ï¸  [RAG] No documents found for query")
            return ""
        
        docs = get_knowledge_docs_by_ids(doc_ids)
        print(f"ğŸ” [RAG] Retrieved {len(docs)} documents from database")

        # spot_code í•„í„°ë§ì´ í•„ìš”í•œ ê²½ìš°
        # spot_idë¡œ í•„í„°ë§í•˜ë ¤ë©´ spots í…Œì´ë¸”ê³¼ ì¡°ì¸ í•„ìš”
        # í˜„ì¬ëŠ” ëª¨ë“  ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ì‚¬ìš© (í–¥í›„ spot_code í•„í„°ë§ ê°œì„  ê°€ëŠ¥)
        # TODO: spot_codeë¡œ í•„í„°ë§í•˜ë ¤ë©´ spots í…Œì´ë¸” ì¡°ì¸ í•„ìš”

        # text í•„ë“œ ì¶”ì¶œ (knowledge_docsëŠ” 'text' í•„ë“œ ì‚¬ìš©)
        texts = []
        for d in docs:
            if isinstance(d, dict) and "text" in d:
                texts.append(d["text"])
        
        context_blob = "\n\n".join(texts)
        print(f"ğŸ“ [RAG] Context length: {len(context_blob)} characters")
        return context_blob
    
    except Exception as e:
        # RAG ì˜¤ë¥˜ ë°œìƒ ì‹œ LLM-only ëª¨ë“œë¡œ ìë™ ì „í™˜
        print(f"âš ï¸  RAG ì˜¤ë¥˜ ë°œìƒ (LLM-only ëª¨ë“œë¡œ ì „í™˜): {type(e).__name__}: {e}")
        return ""


def build_spot_intro_text(spot_code: str, language: str = "en") -> str:
    """
    DBì—ì„œ spot_codeì— í•´ë‹¹í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸ë¥¼ ê°€ì ¸ì™€ì„œ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ í•©ì¹¨.
    """
    scripts = get_scripts_for_spot_code(spot_code)
    
    if not scripts:
        return ""
    
    # text_en í•„ë“œì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    texts = [s["text_en"] for s in scripts]
    return " ".join(texts)


def _truncate_to_two_sentences(text: str) -> str:
    """
    ë‹µë³€ì„ ìµœëŒ€ 2ë¬¸ì¥ìœ¼ë¡œ ì œí•œí•˜ëŠ” ì•ˆì „ì¥ì¹˜.
    """
    import re
    if not text or not text.strip():
        return text
    
    # ë¬¸ì¥ êµ¬ë¶„ìë¡œ ë¶„ë¦¬ (. ! ?)
    # íŒ¨í„´: ë¬¸ì¥ ë êµ¬ë¶„ì(. ! ?)ì™€ ê·¸ ë’¤ì˜ ê³µë°±ì„ í¬í•¨
    sentences = re.split(r'([.!?]+\s*)', text)
    
    # êµ¬ë¶„ìì™€ ë¬¸ì¥ì„ ë‹¤ì‹œ ê²°í•©í•˜ì—¬ ìµœëŒ€ 2ë¬¸ì¥ë§Œ ì¶”ì¶œ
    result = []
    i = 0
    while i < len(sentences) and len(result) < 2:
        sentence_part = sentences[i].strip()
        if sentence_part:
            # ë‹¤ìŒ ìš”ì†Œê°€ êµ¬ë¶„ìë©´ ê²°í•©
            if i + 1 < len(sentences):
                full_sentence = sentence_part + sentences[i + 1]
                result.append(full_sentence.strip())
                i += 2
            else:
                result.append(sentence_part)
                i += 1
        else:
            i += 1
    
    if result:
        return ' '.join(result).strip()
    return text.strip()


def _is_potentially_unclear(question: str) -> bool:
    """
    Detect if a question might be unclear or mis-transcribed.
    This helps the LLM handle ASR errors better.
    """
    if not question or len(question.strip()) < 3:
        return True
    
    # Check for common ASR error patterns
    import re
    
    # Very short questions might be incomplete
    words = question.split()
    if len(words) < 2:
        return True
    
    # Check for excessive non-alphanumeric characters
    non_alnum_ratio = len(re.sub(r'[a-zA-Z0-9ê°€-í£\s]', '', question)) / max(len(question), 1)
    if non_alnum_ratio > 0.3:
        return True
    
    # Check for words that look like gibberish (very long words, repeated patterns)
    for word in words:
        if len(word) > 20:  # Unusually long word
            return True
        if re.search(r'(.)\1{4,}', word):  # Excessive character repetition
            return True
    
    return False


def build_llm_prompt_for_qa(
    spot_code: str,
    user_question: str,
    place_id: str = "gyeongbokgung",
    language: str = "en",
) -> str:
    """
    RAG ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ LLM í”„ë¡¬í”„íŠ¸ ìƒì„±.
    í•­ìƒ ì˜ì–´ë¡œ ë‹µë³€í•˜ë„ë¡ ìš”ì²­ (ë²ˆì—­ì€ ë³„ë„ë¡œ ì²˜ë¦¬).
    
    Args:
        spot_code: ìŠ¤íŒŸ ì½”ë“œ
        user_question: ì‚¬ìš©ì ì§ˆë¬¸ (ì˜ì–´)
        place_id: ì¥ì†Œ ID
        language: ì–¸ì–´ ì½”ë“œ (í˜„ì¬ ë¯¸ì‚¬ìš©, í•­ìƒ ì˜ì–´ë¡œ ë‹µë³€)
    
    Returns:
        LLM í”„ë¡¬í”„íŠ¸ ë¬¸ìì—´
        
    Note:
        - ENABLE_RAGê°€ Trueì´ë©´ RAG ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
        - ENABLE_RAGê°€ Falseì´ë©´ ì»¨í…ìŠ¤íŠ¸ ì—†ì´ ì¼ë°˜ í”„ë¡¬í”„íŠ¸ ìƒì„±
        - Unclear transcriptions are automatically detected and handled
    """
    # Check if question might be unclear
    is_unclear = _is_potentially_unclear(user_question)
    # ========================================================================
    # RAG CONTEXT RETRIEVAL
    # ========================================================================
    # build_rag_context() will return empty string if ENABLE_RAG = False
    # This allows seamless switching between RAG and non-RAG modes
    context = build_rag_context(user_question, spot_code=spot_code)
    
    # ========================================================================
    # PROMPT GENERATION: RAG Mode (with context)
    # ========================================================================
    if context:
        print(f"âœ… [RAG] Using RAG mode with context ({len(context)} chars)")
        # RAG is enabled and context was successfully retrieved
        unclear_instruction = ""
        if is_unclear:
            unclear_instruction = (
                "\nIMPORTANT: The user's question may be unclear or mis-transcribed due to speech recognition errors.\n"
                "- If the question doesn't make sense, try to interpret it in the context of Gyeongbokgung Palace.\n"
                "- If you cannot understand the question at all, say: 'I didn't catch that. Could you please repeat your question?'\n"
                "- Otherwise, answer based on what you think the user might be asking about.\n"
            )
        
        prompt = (
            "You are Dori, a multilingual tour guide robot.\n"
            "PREFER the given context to answer the user's question, but if the answer is not in the context, use your general knowledge to answer.\n"
            "CRITICAL RULES:\n"
            "- Answer in exactly 2 sentences or less\n"
            "- Include ONLY the most essential information\n"
            "- Stop immediately after answering - do not add any additional sentences\n"
            "- First try to answer using the provided context\n"
            "- If the answer is not in the context, use your general knowledge to provide an answer\n"
            "- Answer in English"
            + unclear_instruction +
            "\n\n"
            f"[Context]\n{context}\n\n"
            f"[Question]\n{user_question}\n\n"
            "[Answer in English (2 sentences max, essential info only)]:"
        )
    # ========================================================================
    # PROMPT GENERATION: Non-RAG Mode (without context)
    # ========================================================================
    else:
        # Either RAG is disabled (ENABLE_RAG = False) or no context was found
        # In this mode, LLM relies on its general knowledge
        print(f"â„¹ï¸  [RAG] Using non-RAG mode (no context)")
        unclear_instruction = ""
        if is_unclear:
            unclear_instruction = (
                "\nIMPORTANT: The user's question may be unclear or mis-transcribed due to speech recognition errors.\n"
                "- If the question doesn't make sense, try to interpret it in the context of Gyeongbokgung Palace.\n"
                "- If you cannot understand the question at all, say: 'I didn't catch that. Could you please repeat your question?'\n"
                "- Otherwise, answer based on what you think the user might be asking about.\n"
            )
        
        prompt = (
            "You are Dori, a multilingual tour guide robot.\n"
            "CRITICAL RULES:\n"
            "- Answer in exactly 2 sentences or less\n"
            "- Include ONLY the most essential information\n"
            "- Stop immediately after answering - do not add any additional sentences\n"
            "- If you don't know, say only: 'I don't have that information.'\n"
            "- Answer in English"
            + unclear_instruction +
            "\n\n"
            f"[Question]\n{user_question}\n\n"
            "[Answer in English (2 sentences max, essential info only)]:"
        )
    
    return prompt
